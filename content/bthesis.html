<div class="project-page">
    <div class="project-intro">
      <div class="meta">
        <p class="project-title">A comaparative analysis of Generative Paradigms for text-to-image synthesis</p>
        <p class="project-creators">By Samir Rajesh</p>
      </div>
      <div class="artistic">
        <p>
          My undergraduate thesis project at the University of Warwick. Conducted entirely on a hobbyist workstation, the research investigates and compares two leading approaches for generating images from text prompts: Diffusion Models (DDPM) and Generative Adversarial Networks (GANs). I implemented and trained both unconditional and class-conditioned variants from scratch, evaluating them using Fréchet Inception Distance (FID) scores on the FashionMNIST dataset. Beyond comparing performance, the work documents the end-to-end process of model design, implementation, and training in a non–state-of-the-art compute environment. I’ve included the project’s abstract below.
        </p>
      </div>
    </div>
  
    <div class="project-details">
      <p>
        This is the abstract for the research paper. If you wish to view the code or read the full dissertation, you can find it <a href="https://github.com/samirrajes/dissertation-project" target="_blank">here</a>.
      </p>

      <br>
  
      <p>
        We present a comparative analysis of two generative paradigms for image synthesis: Diffusion Models and Generative Adversarial Networks. We dissect the architectural complexities of these models and develop an understanding of their functionality in image synthesis. We use both quantitative and qualitative methodologies in our analyses, measuring image fidelity using Fréchet Inception Distance. According to our findings, unconditional diffusion attained an FID score of 26.14, and conditional diffusion improved to 21.11. Meanwhile, unconditional and conditional GANs produced scores of 14.76 and 11.46, respectively, representing a disparity from recent literature where DDPMs typically outperform GANs, prompting further investigation. We developed the foundational models—an unconditional diffusion model and an unconditional deep convolutional GAN—extending them to class-conditioned models. We find that conditional generation notably enhanced image quality. A significant achievement of this work is the development, training, and gained understanding of generative paradigms from scratch, all executed on a hobbyist workstation with limited compute resources. This constraint shaped both architectural choices and training schedules, offering insight into what is possible without state-of-the-art hardware. Despite these limitations, our thesis provides a comprehensive dive into the theory and development of generative paradigms, establishing a foundation for future work to bridge the gap observed in performance metrics.
      </p>
  
      <br>

      <p>
        <strong>Keywords:</strong> Diffusion Models, Generative Adversarial Networks, Convolutional Neural Networks, Image synthesis, Machine Learning, Deep Learning, Comparative analysis, Python
      </p>
    </div>
  </div>
  